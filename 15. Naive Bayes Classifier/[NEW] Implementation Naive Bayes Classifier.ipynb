{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we will do first**\n",
    "- let say we have X matrix with columns as features and Y vector\n",
    "- Y contains classes like a1, a2, a3....\n",
    "- We will create a dictionary to implement naive bayes formula for classes\n",
    "- dictionary will contain ai i.e classes as key within this class we will store for each of the feature, what are all possible values that feature can take then storing their count i.e how many data in your feature belongs to this class.\n",
    "\n",
    "$[a_{1}]\\rightarrow [j]\\rightarrow [x^{j}_{1}]...[x^{j}_{k}]\\rightarrow count$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, Y_train):\n",
    "    result = {}\n",
    "    class_values = set(Y_train) # to get number of classes\n",
    "    for current_class in class_values:\n",
    "        result[current_class] = {}\n",
    "        result[\"total_data\"] = len(Y_train)\n",
    "        current_class_rows = (Y_train == current_class)\n",
    "        X_train_current = X_train[current_class_rows]\n",
    "        Y_train_current = Y_train[current_class_rows]\n",
    "        num_features = X_train.shape[1] # no. of features\n",
    "        result[current_class][\"total_count\"] = len(Y_train_current)\n",
    "        for j in range(1, num_features + 1):\n",
    "            j_1 = j- 1\n",
    "            result[current_class][j] = {}\n",
    "            all_possible_values = set(X_train[:, j-1]) # set filters unqiue elements\n",
    "            for current_value in all_possible_values:\n",
    "                result[current_class][j][current_value] = (X_train_current[:, j - 1] == current_value).sum()\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_features = 5\n",
    "# for j in range(num_features):\n",
    "#     print(j)\n",
    "# print(end=\"\\n\")\n",
    "# for j in range(1,num_features+1):\n",
    "#     print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(dictionary, x):\n",
    "    classes = dictionary.keys()\n",
    "    beat_P = -1000\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    for current_class in classes:\n",
    "        if(current_class == \"total_data\"):\n",
    "            continue\n",
    "        p_current_class = probability(dictionary,x,current_class)\n",
    "        if(first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary, x, current_class):\n",
    "    output = np.log(dictionary[current_class][\"total_count\"])- np.log(dictionary['total_data'])\n",
    "    num_features = len(dictionary[current_class].keys())-1\n",
    "    for j in range(1, num_features+1):\n",
    "        xj = x[j-1]\n",
    "        count_current_class_with_value_xj = dictionary[current_class][j][xj] + 1 # laplace correction\n",
    "        count_current_class = dictionary[current_class]['total_count'] + len(dictionary[current_class][j].keys()) #laplace correction deno\n",
    "        current_xj_probability = np.log(count_current_class_with_value_xj)-np.log(count_current_class)\n",
    "        output = output + current_xj_probability\n",
    "        \"\"\"\n",
    "        p'() + pi p() will give very large value\n",
    "        to fix it we will take log of both the terms and will add them\n",
    "        then it will become like:\n",
    "        log p' + log p1 + log p2 ....\n",
    "        \n",
    "        \"\"\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dictionary, X_test):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        x_class = predictSinglePoint(dictionary,x)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "iris dataset will be having continous data likes 1.32, 1.21..\n",
    "we want it to be a labelled data, to do it pick the mean of all \n",
    "possible value as point and then mean/2 and mean/3 as another points other point \n",
    "and we will label them.\n",
    "\"\"\"\n",
    "def makeLabelled(column):\n",
    "    second_limit = column.mean()\n",
    "    first_limit = 0.5 * second_limit\n",
    "    third_limit = 1.5 * second_limit\n",
    "    for i in range(0, len(column)):\n",
    "        if(column[i]<first_limit):\n",
    "            column[i] = 0\n",
    "        elif(column[i]<second_limit):\n",
    "            column[i] = 1\n",
    "        elif(column[i]<third_limit):\n",
    "            column[i] = 2\n",
    "        else:\n",
    "            column[i] = 3\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, X.shape[-1]):\n",
    "    X[:,i] = makeLabelled(X[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(X,Y,test_size = 0.25, random_state=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary =  fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = predict(dictionary,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        18\n",
      "           1       0.89      1.00      0.94         8\n",
      "           2       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.96      0.97      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n",
      "[[18  0  0]\n",
      " [ 0  8  0]\n",
      " [ 0  1 11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "attachments": {
    "0258.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to deal with continous datasets\n",
    "- We assumes that dataset follows the gaussian curve\n",
    "![0258.png](attachment:0258.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        18\n",
      "           1       0.73      1.00      0.84         8\n",
      "           2       1.00      0.75      0.86        12\n",
      "\n",
      "    accuracy                           0.92        38\n",
      "   macro avg       0.91      0.92      0.90        38\n",
      "weighted avg       0.94      0.92      0.92        38\n",
      "\n",
      "[[18  0  0]\n",
      " [ 0  8  0]\n",
      " [ 0  3  9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
